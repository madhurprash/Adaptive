# Agent Optimization Feature Rollout Planning Prompt

## Context
You are an AI agent optimization expert tasked with analyzing an agentic system and recommending improvements based on industry best practices for context engineering, agent architecture, and human-in-the-loop optimization workflows.

## Your Task
Analyze the self-healing-agent codebase located at `/Users/madhurpt/Desktop/self-healing-agent` and generate a prioritized feature rollout plan for agent optimization improvements. Focus on both **offline optimization** (system prompt refinement, tool optimization, examples) and **online optimization** (runtime context engineering, adaptive behaviors, real-time learning).

## Methodology

### Step 1: Codebase Analysis
Scan the entire repository and understand:
1. **Current agent architecture** - How agents are structured, their roles, and interaction patterns
2. **System prompts** - Location, structure, and effectiveness of existing prompts
3. **Tool definitions** - Quality of tool descriptions, examples, and usage patterns
4. **Context engineering implementations** - Current middleware, summarization, and context management strategies
5. **Message history management** - How conversation context is maintained and optimized
6. **Error handling and recovery** - Current approaches to handling failures
7. **Human-in-the-loop touchpoints** - Where human feedback could improve the system
8. **Testing and validation** - Current quality assurance mechanisms

### Step 2: Best Practices Review
Review and apply insights from these key resources:

#### Context Engineering Best Practices
- **Anthropic: Effective Context Engineering for AI Agents**
  - URL: https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents
  - Focus areas: Context window management, information prioritization, adaptive context strategies

- **Anthropic: Writing Tools for Agents**
  - URL: https://www.anthropic.com/engineering/writing-tools-for-agents
  - Focus areas: Tool description quality, parameter design, example effectiveness

- **LangChain: Context Engineering Guide**
  - URL: https://docs.langchain.com/oss/python/langchain/context-engineering
  - Focus areas: RAG patterns, context compaction, semantic chunking

#### Additional Considerations
- **Sub-agent architecture patterns** - When to delegate to specialized agents
- **Structured note-taking systems** - How to maintain agent "memory" effectively
- **Data analysis and observability** - Runtime monitoring and optimization loops
- **Token budget optimization** - Cost-effective context management strategies

### Step 3: Gap Analysis
Compare the current implementation against best practices:
1. **What's working well?** - Identify strengths to preserve
2. **What's missing?** - Identify critical gaps
3. **What's suboptimal?** - Identify areas for improvement
4. **What's over-engineered?** - Identify unnecessary complexity

### Step 4: Feature Recommendation
For each improvement opportunity, provide:

#### Offline Optimization Features
Features that require human review and approval before deployment:

**Format:**
```
FEATURE: [Name]
CATEGORY: Offline - [System Prompt | Tool Design | Examples | Architecture]
PRIORITY: [P0-Critical | P1-High | P2-Medium | P3-Low]
IMPACT: [Accuracy | Latency | Cost | UX | Reliability]

PROBLEM:
[Clear description of what's not optimal currently]

SOLUTION:
[Specific, actionable improvement]

IMPLEMENTATION:
- [ ] Step 1
- [ ] Step 2
- [ ] Step 3

HUMAN-IN-THE-LOOP WORKFLOW:
1. [How an LLM would propose this change]
2. [What a human needs to review/approve]
3. [How to test/validate the change]
4. [Rollback strategy if issues occur]

FILES TO MODIFY:
- /path/to/file1.py (lines X-Y)
- /path/to/prompt.txt (section Z)

SUCCESS METRICS:
- Metric 1: [How to measure improvement]
- Metric 2: [How to measure no regression]

REFERENCES:
- [Link to relevant best practice article]
- [Specific section or principle applied]
```

#### Online Optimization Features
Features that enable runtime adaptation and learning:

**Format:**
```
FEATURE: [Name]
CATEGORY: Online - [Context Mgmt | Adaptive Behavior | Monitoring | Learning]
PRIORITY: [P0-Critical | P1-High | P2-Medium | P3-Low]
IMPACT: [Accuracy | Latency | Cost | UX | Reliability]

PROBLEM:
[What runtime behavior could be optimized]

SOLUTION:
[How to enable adaptive optimization]

IMPLEMENTATION:
- [ ] Data collection mechanism
- [ ] Analysis/learning component
- [ ] Adaptation trigger logic
- [ ] Validation and safety checks

HUMAN-IN-THE-LOOP WORKFLOW:
1. [What signals/errors trigger review]
2. [How the system presents findings to human]
3. [What decisions require human approval]
4. [How approved changes get deployed]

MONITORING:
- What to track: [Metrics to monitor]
- When to alert: [Thresholds for human intervention]
- Dashboard view: [How to visualize behavior]

SUCCESS METRICS:
- Metric 1: [Improvement measurement]
- Metric 2: [Safety/stability measurement]

REFERENCES:
- [Link to relevant best practice article]
- [Specific section or principle applied]
```

### Step 5: Prioritization
Rank features using this framework:

**P0 - Critical (Do First):**
- Fixes critical bugs or prevents catastrophic failures
- Unblocks other high-priority work
- Required for production readiness

**P1 - High (Do Soon):**
- Significant impact on accuracy or user experience
- Enables important new capabilities
- Addresses frequent pain points

**P2 - Medium (Do Eventually):**
- Incremental improvements to existing features
- Nice-to-have enhancements
- Optimization opportunities

**P3 - Low (Do If Time Permits):**
- Minor improvements
- Edge case handling
- Polish and refinement

## Output Format

### Executive Summary
[2-3 paragraph overview of findings]

### Current State Assessment
**Strengths:**
- Strength 1
- Strength 2

**Weaknesses:**
- Weakness 1
- Weakness 2

**Opportunities:**
- Opportunity 1
- Opportunity 2

### Recommended Feature Rollout

#### Phase 1: Foundation (Weeks 1-2)
**P0 Critical Features:**
1. [Feature name] - [1 sentence impact]
2. [Feature name] - [1 sentence impact]

**Rationale:** [Why these must go first]

#### Phase 2: Core Improvements (Weeks 3-5)
**P1 High-Priority Features:**
1. [Feature name] - [1 sentence impact]
2. [Feature name] - [1 sentence impact]

**Rationale:** [Why these come next]

#### Phase 3: Enhancement (Weeks 6-8)
**P2 Medium-Priority Features:**
1. [Feature name] - [1 sentence impact]
2. [Feature name] - [1 sentence impact]

**Rationale:** [Value proposition]

#### Phase 4: Polish (Weeks 9+)
**P3 Low-Priority Features:**
1. [Feature name] - [1 sentence impact]

**Rationale:** [Nice-to-haves]

### Detailed Feature Specifications
[Full feature cards for each recommendation using templates above]

### Implementation Roadmap
```mermaid
gantt
    title Agent Optimization Rollout
    dateFormat  YYYY-MM-DD
    section Phase 1
    Feature 1           :2024-01-01, 7d
    Feature 2           :2024-01-08, 7d
    section Phase 2
    Feature 3           :2024-01-15, 14d
    ...
```

### Risk Assessment
| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Risk 1 | High/Med/Low | High/Med/Low | Strategy |

### Success Criteria
**Phase 1 Success Metrics:**
- [ ] Metric 1: Target
- [ ] Metric 2: Target

**Phase 2 Success Metrics:**
- [ ] Metric 3: Target
- [ ] Metric 4: Target

### Appendix: Reference Implementations
[Code snippets demonstrating key patterns from best practice articles]

---

## Special Instructions

1. **Be Specific**: Don't say "improve prompts" - say "add 3 few-shot examples to the insights agent prompt showing error pattern analysis"

2. **Be Actionable**: Every recommendation should have clear implementation steps

3. **Reference Best Practices**: Always cite which principle/article inspired each recommendation

4. **Consider Trade-offs**: Acknowledge costs (latency, tokens, complexity) of each improvement

5. **Human-in-the-Loop First**: Every feature must include clear HITL workflow

6. **Safety-First**: Include validation, rollback, and monitoring in every feature

7. **Quantify Impact**: Estimate improvement percentages when possible (e.g., "reduce context size by 30%")

8. **Leverage Existing Patterns**: Build on the middleware architecture and tool patterns already in place

9. **Incremental Evolution**: Prefer many small, testable improvements over large rewrites

10. **Documentation**: Every feature should include inline code comments and README updates

---

## Analysis Scope

### Files to Definitely Review
```
/agent.py - Main agent logic
/agent_middleware/*.py - Context engineering
/agent_tools/*.py - Tool definitions
/prompt_templates/**/*.txt - All system prompts
/config.yaml - Configuration
/design_notes/*.md - Design philosophy
```

### Key Questions to Answer
1. How effective are the current system prompts? What's missing?
2. Are tool descriptions clear enough? Do they have good examples?
3. Is context management optimal? Are there better strategies?
4. Where could sub-agents add value?
5. What errors occur most frequently? How can we prevent them?
6. Where should humans be in the loop?
7. What runtime data should we collect for optimization?
8. How can we make the evolution agent more effective when built?

---

## Deliverable

A comprehensive, actionable feature rollout plan that:
- ✅ Is grounded in the current codebase reality
- ✅ Applies proven best practices from the reference articles
- ✅ Includes clear human-in-the-loop workflows
- ✅ Balances offline (review-based) and online (adaptive) optimization
- ✅ Has specific, testable success metrics
- ✅ Provides implementation guidance down to file/line level
- ✅ Considers costs, risks, and trade-offs
- ✅ Enables continuous improvement over time

---

## Example Usage

```bash
# Run this prompt to get a feature rollout plan
cat feature_rollout/feature_rollout_plan.txt | claude-code

# Or paste this entire prompt into Claude Code with:
# "Generate a feature rollout plan for agent optimization based on the prompt in feature_rollout/feature_rollout_plan.txt"
```

---

## Version History
- v1.0 (2024-01-12): Initial prompt template
- [Future versions will track best practice updates]
