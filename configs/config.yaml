general:
  name: "adaptive"
  description: "This is the configuration file that contains information about the models and infra used by the adaptive agent"

# This contains the model information for various agents
# in our multi agentic system
insights_agent_model_information:
  # Platform-specific prompt templates
  prompts:
    # LangSmith-specific prompt (original)
    langsmith: prompt_templates/insights_prompts/insights_prompt_langsmith.txt
    # Langfuse-specific prompt (new, tailored to Langfuse trace structure)
    langfuse: prompt_templates/insights_prompts/insights_prompt_langfuse.txt
    # MLflow-specific prompt (tailored to Databricks MLflow trace structure)
    mlflow: prompt_templates/insights_prompts/insights_prompt_mlflow.txt
  # Represents the model id that the agent will use within the system
  # as an deep agent model
  model_id: us.anthropic.claude-3-5-sonnet-20240620-v1:0
  # Represents the inference parameters that the foundation model
  # uses at runtime during inference
  inference_parameters:
    temperature: 0.1
    max_tokens: 2048
    top_p: 0.92
    caching: true

# This section contains information used by the main
# agent to implement efficient context engineering of
# model context and tool results
context_engineering_info:
  # Summarization middleware information
  # this is to automatically summarize conversation
  # history when approaching token limits
  summarization_middleware: 
    # this is the model that will be used as a summarization
    # model
    model_id: us.anthropic.claude-3-5-haiku-20241022-v1:0
    # these ate the number of messages to keep after the summarization
    messages_to_keep: 20
    # temperature used for the summarization middleware
    temperature: 0.1
    # this is the token threshold at which the summarization is instantiated
    max_tokens_before_summary: 4000
    # max tokens
    max_tokens: 2000
    # This is the path to the summarization prompt
    summary_prompt: prompt_templates/context_engineering/langsmith/summarization_base_prompt.txt
    summary_prefix: "Previous conversation summary"

  # Token threshold for context management
  # Used by both TokenLimitCheckMiddleware and ToolResponseSummarizer
  # to ensure consistent behavior across all context management components
  # When messages or tool responses exceed this threshold, they are summarized
  token_threshold: 100000  # 100k tokens

# Deep Research Agent Configuration
# This agent analyzes error patterns from insights AND performs internet research
# to find solutions and best practices - it's a single unified agent
deep_research_agent_model_information:
  # Prompt template for the deep research agent (handles both error analysis and internet search)
  deep_research_agent_prompt: prompt_templates/evolution_error_analysis.txt
  # Model ID for deep research agent
  model_id: us.anthropic.claude-sonnet-4-20250514-v1:0
  # Inference parameters for deep research
  inference_parameters:
    temperature: 0.1
    max_tokens: 8192
    top_p: 0.92
    caching: true
  # Internet search configuration
  internet_search:
    max_results: 5
    topic: "general"  # Options: general, news, finance
    include_raw_content: false
  # Output configuration
  output:
    default_output_dir: "reports"
    default_file_format: "md"  # markdown format
  # Agent repository access configuration
  # The deep research agent can access either a local repository or clone from GitHub
  agent_repository:
    type: "local"  # Options: "local" or "github"
    local_path: "/Users/madhurpt/Desktop/adaptive"  # Path to local agent repository
    github_url: ""  # GitHub URL if type is "github" (e.g., https://github.com/user/repo)

# NOTE: Routing configuration has been replaced by the orchestrator agent
# The orchestrator handles all routing decisions intelligently
# See 'orchestrator_agent' section below for orchestrator configuration

# Orchestrator Agent Configuration
# The orchestrator is the main agent that decides which sub-agents to invoke
orchestrator_agent:
  # Orchestrator prompt template - describes architecture and routing decisions
  prompt_template_path: prompt_templates/orchestrator_prompt.txt
  # Model for orchestrator (Sonnet 4 for intelligent routing)
  model_id: us.anthropic.claude-sonnet-4-20250514-v1:0
  # Inference parameters
  inference_parameters:
    temperature: 0.3
    max_tokens: 2048
    top_p: 0.92
    caching: true
  # Sub-agents that the orchestrator can route to
  available_subagents:
    - name: "insights"
      description: "Analyzes agent execution traces from observability platforms (LangSmith, Langfuse, MLflow)"
      when_to_use: "User asks about agent behavior, errors, performance, or needs trace analysis"
    - name: "evolution"
      description: "Optimizes and improves agent prompts based on error patterns and performance issues"
      when_to_use: "User wants to improve/optimize prompts, or insights reveal performance issues"
    - name: "generate_tasks"
      description: "Generates synthetic test cases to identify capability gaps and untested scenarios"
      when_to_use: "User explicitly requests task generation or wants to test agent capabilities"

# AgentCore Memory Configuration
# Manages long-term conversation memory with semantic search and context retrieval
agentcore_memory:
  # Enable or disable AgentCore Memory (set to false to use in-memory only)
  enabled: true
  # Memory name - unique identifier for this agent's memory
  memory_name: "AdaptiveMemory"
  # Description of the memory purpose
  description: "Conversation memory for Adaptive agent with error analysis and insights"
  # AWS region for AgentCore Memory service
  region_name: "us-west-2"
  # IAM role ARN with permissions for memory operations
  # Must have bedrock:* permissions and trust policy for AgentCore Memory
  memory_execution_role_arn: "arn:aws:iam::218208277580:role/BedrockAgentCoreMemoryExecutionRole"  # Set via environment variable: AGENTCORE_MEMORY_ROLE_ARN
  # Model ID for memory extraction and consolidation
  memory_model_id: "us.anthropic.claude-3-5-sonnet-20240620-v1:0"
  # Context retrieval settings
  context_retrieval:
    # Number of most relevant messages to retrieve from memory
    top_k_relevant: 3
    # Number of most recent raw messages to keep in context
    keep_recent_messages: 3
    # Maximum messages to store per user before triggering consolidation
    max_messages_before_consolidation: 50

# Self-Questioning Configuration
# Enables autonomous synthetic task generation for continuous agent improvement
self_questioning:
  # Enable or disable self-questioning (set to true to enable automatic task generation)
  enabled: true
  # Auto-trigger after insights generation (runs automatically if enabled)
  auto_trigger_after_insights: true
  # Self-questioning agent configuration
  self_questioning_agent:
    model_id: us.anthropic.claude-sonnet-4-20250514-v1:0
    inference_parameters:
      temperature: 0.7  # Higher temperature for diverse task generation
      max_tokens: 4096
      top_p: 0.92
      caching: true
  # Capability gap analyzer configuration
  capability_gap_analyzer:
    model_id: us.anthropic.claude-3-5-sonnet-20240620-v1:0
    inference_parameters:
      temperature: 0.3  # Lower temperature for analytical reasoning
      max_tokens: 2048
      top_p: 0.92
    min_traces_for_analysis: 5
    trace_analysis_window_days: 7
  # Task generator configuration
  task_generator:
    max_tasks_per_session: 5  # Reduced for inline generation (use CLI for more)
    difficulty_distribution:
      easy: 0.3
      medium: 0.5
      hard: 0.2
    task_type_weights:
      exploration: 0.3
      edge_case: 0.3
      optimization: 0.2
      regression: 0.2
  # Diversity enforcer configuration
  diversity_enforcer:
    embedding_model: "amazon.titan-embed-text-v2:0"
    min_similarity_threshold: 0.85
    embedding_dimensions: 1024
    cache_size: 100
  # Task storage configuration
  task_storage:
    memory_namespace: "synthetic_tasks"
    max_tasks_per_user: 1000
    task_expiration_days: 30