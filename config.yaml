general:
  name: "evolve.ai"
  description: "This is the configuration file that contains information about the models and infra used by the evolution agent"

# This contains the model information for various agents
# in our multi agentic system
insights_agent_model_information:
  # Observability platform to use (langsmith or langfuse)
  # Leave empty to auto-detect from environment variables or user question
  observability_platform: ""  # Options: "langsmith", "langfuse", or "" for auto-detect
  # this is the prompt template that the insights agent uses (framework-agnostic)
  insights_agent_prompt: prompt_templates/log_curator_base_prompt.txt
  # Represents the model id that the agent will use within the system
  # as an deep agent model
  model_id: us.anthropic.claude-3-5-sonnet-20240620-v1:0
  # Represents the inference parameters that the foundation model
  # uses at runtime during inference
  inference_parameters:
    temperature: 0.1
    max_tokens: 2048
    top_p: 0.92
    caching: true

# This section contains information used by the main
# agent to implement efficient context engineering of
# model context and tool results
context_engineering_info:
  # Summarization middleware information
  # this is to automatically summarize conversation
  # history when approaching token limits
  summarization_middleware: 
    # this is the model that will be used as a summarization
    # model
    model_id: us.anthropic.claude-3-5-haiku-20241022-v1:0
    # these ate the number of messages to keep after the summarization
    messages_to_keep: 20
    # temperature used for the summarization middleware
    temperature: 0.1
    # this is the token threshold at which the summarization is instantiated
    max_tokens_before_summary: 4000
    # max tokens
    max_tokens: 2000
    # This is the path to the summarization prompt
    summary_prompt: prompt_templates/context_engineering/langsmith/summarization_base_prompt.txt
    summary_prefix: "Previous conversation summary"

  # Token threshold for context management
  # Used by both TokenLimitCheckMiddleware and ToolResponseSummarizer
  # to ensure consistent behavior across all context management components
  # When messages or tool responses exceed this threshold, they are summarized
  token_threshold: 100000  # 100k tokens

# Deep Research Agent Configuration
# This agent analyzes error patterns from insights AND performs internet research
# to find solutions and best practices - it's a single unified agent
deep_research_agent_model_information:
  # Prompt template for the deep research agent (handles both error analysis and internet search)
  deep_research_agent_prompt: prompt_templates/evolution_error_analysis.txt
  # Model ID for deep research agent
  model_id: us.anthropic.claude-sonnet-4-20250514-v1:0
  # Inference parameters for deep research
  inference_parameters:
    temperature: 0.1
    max_tokens: 8192
    top_p: 0.92
    caching: true
  # Internet search configuration
  internet_search:
    max_results: 5
    topic: "general"  # Options: general, news, finance
    include_raw_content: false
  # Output configuration
  output:
    default_output_dir: "reports"
    default_file_format: "md"  # markdown format
  # Agent repository access configuration
  # The deep research agent can access either a local repository or clone from GitHub
  agent_repository:
    type: "local"  # Options: "local" or "github"
    local_path: "/Users/madhurpt/Desktop/self-healing-agent"  # Path to local agent repository
    github_url: ""  # GitHub URL if type is "github" (e.g., https://github.com/user/repo)

# Routing Configuration
# Controls conditional routing between insights agent and deep research agent
routing_configuration:
  # Small, fast model for routing decisions
  router_model_id: us.anthropic.claude-3-5-haiku-20241022-v1:0
  # Inference parameters for router
  inference_parameters:
    temperature: 0.1
    max_tokens: 100
    top_p: 0.92
  # Routing prompt template path
  router_prompt_path: prompt_templates/routing_decision_prompt.txt

# Platform Router Configuration
# Dynamically determines which observability platform to use based on user question
platform_router_configuration:
  # Enable dynamic platform routing (if false, uses insights_agent_model_information.observability_platform)
  enabled: true
  # Enable session-based platform persistence (if true, prompts user once at session start)
  # If false, uses the router on every turn to determine platform from user question
  session_based_persistence: true
  # Small, fast model for platform routing decisions
  router_model_id: us.anthropic.claude-3-5-haiku-20241022-v1:0
  # Inference parameters for platform router
  inference_parameters:
    temperature: 0.1
    max_tokens: 50
    top_p: 0.92
  # Platform routing prompt template path
  router_prompt_path: prompt_templates/platform_router_prompt.txt

# AgentCore Memory Configuration
# Manages long-term conversation memory with semantic search and context retrieval
agentcore_memory:
  # Enable or disable AgentCore Memory (set to false to use in-memory only)
  enabled: true
  # Memory name - unique identifier for this agent's memory
  memory_name: "SelfHealingAgentMemory"
  # Description of the memory purpose
  description: "Conversation memory for self-healing agent with error analysis and insights"
  # AWS region for AgentCore Memory service
  region_name: "us-west-2"
  # IAM role ARN with permissions for memory operations
  # Must have bedrock:* permissions and trust policy for AgentCore Memory
  memory_execution_role_arn: "arn:aws:iam::218208277580:role/BedrockAgentCoreMemoryExecutionRole"  # Set via environment variable: AGENTCORE_MEMORY_ROLE_ARN
  # Model ID for memory extraction and consolidation
  memory_model_id: "us.anthropic.claude-3-5-sonnet-20240620-v1:0"
  # Context retrieval settings
  context_retrieval:
    # Number of most relevant messages to retrieve from memory
    top_k_relevant: 3
    # Number of most recent raw messages to keep in context
    keep_recent_messages: 3
    # Maximum messages to store per user before triggering consolidation
    max_messages_before_consolidation: 50